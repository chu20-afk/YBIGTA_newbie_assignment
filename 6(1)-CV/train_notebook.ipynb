{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CIFAR-100 Training: ViT-Small vs ResNet18Small\n",
        "\n",
        "⚠️ 중요: 모든 변수가 정상적으로 초기화되도록 반드시 처음부터 끝까지 (Cell 1 → Cell N) 모든 셀을 순서대로 실행하세요.\n",
        "\n",
        "이 노트북은 **CIFAR-100 데이터셋(100 클래스)**을 사용하여 ViT-Small과 ResNet18Small 모델을 학습시키고, 두 모델의 성능을 비교합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Library Import\n",
        "\n",
        "Make sure to execute cells in order from top to bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.torch import Rearrange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 하이퍼파라미터 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 학습 설정\n",
        "BATCH_SIZE = 64  # 로컬 CPU에서 빠른 학습을 위해 줄임\n",
        "NUM_EPOCHS = 20  # 빠른 실험을 위해 20 에포크로 설정\n",
        "LEARNING_RATE = 1e-3  # 짧은 학습을 위해 learning rate 증가\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "# 디바이스 설정\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# CIFAR-100 정규화 값\n",
        "MEAN = (0.5071, 0.4867, 0.4408)\n",
        "STD = (0.2675, 0.2565, 0.2761)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 데이터셋 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### data augmentation(TODO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 데이터 변환 정의\n",
        "transform_train = transforms.Compose([\n",
        "    #TODO: implement data augmentation\n",
        "\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(MEAN, STD),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 데이터셋 로드\n",
        "print('==> Preparing data..')\n",
        "trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=0)\n",
        "\n",
        "# CIFAR-100 has 100 classes\n",
        "print(f'Training samples: {len(trainset)}, Test samples: {len(testset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 데이터셋 예시 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def denormalize(tensor, mean, std):\n",
        "    \"\"\"\n",
        "    정규화된 텐서를 원본 이미지로 복원하는 함수\n",
        "    \n",
        "    Args:\n",
        "        tensor: 정규화된 이미지 텐서 (C, H, W)\n",
        "        mean: 정규화에 사용된 평균값\n",
        "        std: 정규화에 사용된 표준편차\n",
        "        \n",
        "    Returns:\n",
        "        원본 범위 [0, 1]로 복원된 텐서\n",
        "    \"\"\"\n",
        "    tensor = tensor.clone()\n",
        "    for t, m, s in zip(tensor, mean, std):\n",
        "        t.mul_(s).add_(m)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "def show_dataset_samples(dataset, classes, mean, std, num_samples=10):\n",
        "    \"\"\"\n",
        "    데이터셋에서 무작위 샘플을 시각화하는 함수\n",
        "    \n",
        "    Args:\n",
        "        dataset: 시각화할 데이터셋\n",
        "        classes: 클래스 이름 리스트\n",
        "        mean: 정규화 평균값\n",
        "        std: 정규화 표준편차\n",
        "        num_samples: 표시할 샘플 개수\n",
        "    \"\"\"\n",
        "    # 무작위로 샘플 선택\n",
        "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
        "    \n",
        "    # 그리드 설정 (2행 5열)\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    for idx, sample_idx in enumerate(indices):\n",
        "        img, label = dataset[sample_idx]\n",
        "        \n",
        "        # 정규화 해제\n",
        "        img = denormalize(img, mean, std)\n",
        "        \n",
        "        # 텐서를 numpy로 변환 (C, H, W) -> (H, W, C)\n",
        "        img = img.permute(1, 2, 0).numpy()\n",
        "        \n",
        "        # [0, 1] 범위로 클리핑\n",
        "        img = np.clip(img, 0, 1)\n",
        "        \n",
        "        # 이미지 표시\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].set_title(f'Label: {classes[label]}', fontsize=10)\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('dataset_samples.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print('\\n==> 데이터셋 샘플이 dataset_samples.png로 저장되었습니다.')\n",
        "\n",
        "\n",
        "# 데이터셋 샘플 시각화\n",
        "print('==> 데이터셋 샘플 시각화...')\n",
        "show_dataset_samples(trainset, trainset.classes, MEAN, STD, num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 데이터 증강 (Augmentation) 효과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_augmentation_examples(num_samples=3, augmentations_per_image=5):\n",
        "    \"\"\"\n",
        "    데이터 증강 효과를 시각화하는 함수\n",
        "    \n",
        "    원본 이미지 하나당 여러 번 증강을 적용하여 어떻게 변하는지 보여줍니다.\n",
        "    \n",
        "    Args:\n",
        "        num_samples: 시각화할 원본 이미지 개수\n",
        "        augmentations_per_image: 각 이미지당 생성할 증강 샘플 개수\n",
        "    \"\"\"\n",
        "    # 증강 없는 데이터셋 (비교용)\n",
        "    dataset_no_aug = torchvision.datasets.CIFAR100(\n",
        "        root='./data', train=True, download=False,\n",
        "        transform=transforms.ToTensor()\n",
        "    )\n",
        "    \n",
        "    # 증강 있는 변환 (정규화 제외)\n",
        "    transform_aug_viz = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "    \n",
        "    # 무작위로 이미지 선택\n",
        "    indices = np.random.choice(len(dataset_no_aug), num_samples, replace=False)\n",
        "    \n",
        "    # 그리드 설정 (각 행: 원본 1개 + 증강 N개)\n",
        "    fig, axes = plt.subplots(num_samples, augmentations_per_image + 1, \n",
        "                            figsize=(3 * (augmentations_per_image + 1), 3 * num_samples))\n",
        "    \n",
        "    if num_samples == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for row, img_idx in enumerate(indices):\n",
        "        # 원본 이미지 가져오기\n",
        "        original_img, label = dataset_no_aug[img_idx]\n",
        "        \n",
        "        # 원본 이미지 표시\n",
        "        original_np = original_img.permute(1, 2, 0).numpy()\n",
        "        axes[row, 0].imshow(original_np)\n",
        "        axes[row, 0].set_title(f'Original\\nClass {label}', fontsize=11, fontweight='bold')\n",
        "        axes[row, 0].axis('off')\n",
        "        \n",
        "        # 증강된 이미지들 표시\n",
        "        # PIL 이미지로 변환 (transform 적용을 위해)\n",
        "        from PIL import Image\n",
        "        pil_img = Image.fromarray((original_np * 255).astype(np.uint8))\n",
        "        \n",
        "        for col in range(augmentations_per_image):\n",
        "            # 증강 적용\n",
        "            aug_img = transform_aug_viz(pil_img)\n",
        "            aug_np = aug_img.permute(1, 2, 0).numpy()\n",
        "            \n",
        "            # 증강된 이미지 표시\n",
        "            axes[row, col + 1].imshow(aug_np)\n",
        "            axes[row, col + 1].set_title(f'Aug #{col+1}', fontsize=11)\n",
        "            axes[row, col + 1].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 데이터 증강 효과 시각화\n",
        "print('==> 데이터 증강 효과 시각화...')\n",
        "show_augmentation_examples(num_samples=3, augmentations_per_image=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. 모델 정의\n",
        "\n",
        "### 4.1 ResNet18Small 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet의 기본 빌딩 블록 (BasicBlock)\n",
        "    \n",
        "    ResNet18과 ResNet34에서 사용되는 residual block입니다.\n",
        "    두 개의 3x3 convolution layer로 구성되며, skip connection을 통해\n",
        "    입력을 출력에 더해주는 구조입니다.\n",
        "    \n",
        "    구조:\n",
        "        Input\n",
        "          ├─→ Conv(3×3) → BN → ReLU → Conv(3×3) → BN ─→ +\n",
        "          └────────────────────────────────────────────→ ↓\n",
        "                                                      ReLU → Output\n",
        "    \n",
        "    Args:\n",
        "        in_planes (int): 입력 채널 수\n",
        "        planes (int): 출력 채널 수 (첫 번째 conv의 출력 채널)\n",
        "        stride (int): stride 크기. 2일 경우 feature map 크기가 절반으로 줄어듦\n",
        "    \"\"\"\n",
        "    expansion = 1  # BasicBlock은 채널 수 확장이 없음 (Bottleneck과 구분)\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        \n",
        "        # =========================\n",
        "        # TODO 1) conv1, bn1 정의\n",
        "        # - conv1은 3x3 Conv2d\n",
        "        # - stride는 함수 인자로 받은 stride 사용\n",
        "        # - padding=1, bias=False\n",
        "        # =========================\n",
        "        self.conv1 = ____  # nn.Conv2d(...)\n",
        "        self.bn1 = ____    # nn.BatchNorm2d(...)\n",
        "\n",
        "        # =========================\n",
        "        # TODO 2) conv2, bn2 정의\n",
        "        # - conv2는 3x3 Conv2d\n",
        "        # - stride=1 고정\n",
        "        # - padding=1, bias=False\n",
        "        # =========================\n",
        "        self.conv2 = ____\n",
        "        self.bn2 = ____\n",
        "\n",
        "        # =========================\n",
        "        # TODO 3) shortcut 정의\n",
        "        # - 기본은 nn.Sequential() (identity처럼 동작)\n",
        "        # - 만약 stride != 1 또는 in_planes != planes*self.expansion 이면\n",
        "        #   1x1 Conv2d + BatchNorm2d 로 구성된 nn.Sequential로 교체\n",
        "        # =========================\n",
        "        self.shortcut = ____  # 기본값\n",
        "\n",
        "        # Shortcut connection (skip connection)\n",
        "        # 입력과 출력의 차원이 다를 경우 1x1 conv로 차원 맞춤\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion * planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                ____,  # 1x1 conv: in_planes -> planes*expansion, stride=stride, bias=False\n",
        "                ____   # bn\n",
        "            )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        \n",
        "        Args:\n",
        "            x: 입력 텐서 (batch_size, in_planes, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            출력 텐서 (batch_size, planes, H', W')\n",
        "            H', W'는 stride에 따라 결정됨\n",
        "        \"\"\"\n",
        "        # =========================\n",
        "        # TODO 4) main path 구현\n",
        "        # out = relu(bn1(conv1(x)))\n",
        "        # out = bn2(conv2(out))\n",
        "        # =========================\n",
        "        out = ____\n",
        "        out = ____\n",
        "\n",
        "        # =========================\n",
        "        # TODO 5) skip connection 더하기\n",
        "        # out += shortcut(x)\n",
        "        # =========================\n",
        "        out = ____  # out + shortcut(x) 또는 out += shortcut(x)\n",
        "\n",
        "        # =========================\n",
        "        # TODO 6) 마지막 ReLU\n",
        "        # =========================\n",
        "        out = ____\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNetSmall(nn.Module):\n",
        "    \"\"\"\n",
        "    경량화된 ResNet18 모델 (ResNet18Small)\n",
        "    \n",
        "    원본 ResNet18의 채널 수를 약 37.5%로 줄여 파라미터 수를 \n",
        "    11.17M에서 1.58M으로 대폭 감소시킨 모델입니다.\n",
        "    \n",
        "    채널 구조 비교:\n",
        "        ResNet18:      64 → 64  → 128 → 256 → 512\n",
        "        ResNet18Small: 24 → 24  → 48  → 96  → 192\n",
        "    \n",
        "    구조:\n",
        "        Input (32×32×3)\n",
        "          ↓\n",
        "        Conv1 (3×3, 24 channels)\n",
        "          ↓\n",
        "        Layer1: 2× BasicBlock (24 channels, stride=1)  → 32×32\n",
        "          ↓\n",
        "        Layer2: 2× BasicBlock (48 channels, stride=2)  → 16×16\n",
        "          ↓\n",
        "        Layer3: 2× BasicBlock (96 channels, stride=2)  → 8×8\n",
        "          ↓\n",
        "        Layer4: 2× BasicBlock (192 channels, stride=2) → 4×4\n",
        "          ↓\n",
        "        AvgPool (4×4) → FC (192 → num_classes)\n",
        "    \n",
        "    Args:\n",
        "        block: 사용할 블록 타입 (BasicBlock 또는 Bottleneck)\n",
        "        num_blocks: 각 layer의 블록 개수 리스트 [2,2,2,2]\n",
        "        num_classes: 분류할 클래스 개수 (CIFAR-10: 10, CIFAR-100: 100)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNetSmall, self).__init__()\n",
        "        self.in_planes = 24  # 초기 채널 수 (원본 ResNet18의 64에서 24로 축소)\n",
        "\n",
        "        # 첫 번째 convolution layer\n",
        "        self.conv1 = nn.Conv2d(3, 24, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(24)\n",
        "        \n",
        "        # 4개의 residual layer 생성\n",
        "        self.layer1 = self._make_layer(block, 24, num_blocks[0], stride=1)   # 32×32 유지\n",
        "        self.layer2 = self._make_layer(block, 48, num_blocks[1], stride=2)   # 16×16\n",
        "        self.layer3 = self._make_layer(block, 96, num_blocks[2], stride=2)   # 8×8\n",
        "        self.layer4 = self._make_layer(block, 192, num_blocks[3], stride=2)  # 4×4\n",
        "        \n",
        "        # 최종 classification layer\n",
        "        self.linear = nn.Linear(192 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        \"\"\"\n",
        "        Residual layer를 생성하는 헬퍼 함수\n",
        "        \n",
        "        각 layer는 여러 개의 residual block으로 구성됩니다.\n",
        "        첫 번째 블록에서만 stride를 적용하여 downsampling하고,\n",
        "        나머지 블록들은 stride=1로 feature map 크기를 유지합니다.\n",
        "        \n",
        "        Args:\n",
        "            block: 블록 타입 (BasicBlock)\n",
        "            planes: 출력 채널 수\n",
        "            num_blocks: 이 layer에 포함될 블록 개수\n",
        "            stride: 첫 번째 블록의 stride (downsampling 여부 결정)\n",
        "            \n",
        "        Returns:\n",
        "            nn.Sequential: 여러 블록이 순차적으로 연결된 layer\n",
        "        \"\"\"\n",
        "        strides = [stride] + [1] * (num_blocks - 1)  # [stride, 1, 1, ...]\n",
        "        layers = []\n",
        "        \n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion  # 다음 블록의 입력 채널 수 업데이트\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        \n",
        "        Args:\n",
        "            x: 입력 이미지 텐서 (batch_size, 3, 32, 32)\n",
        "            \n",
        "        Returns:\n",
        "            출력 로짓 (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # Initial convolution\n",
        "        out = F.relu(self.bn1(self.conv1(x)))  # (B, 24, 32, 32)\n",
        "        \n",
        "        # 4개의 residual layer 통과\n",
        "        out = self.layer1(out)  # (B, 24, 32, 32)\n",
        "        out = self.layer2(out)  # (B, 48, 16, 16)\n",
        "        out = self.layer3(out)  # (B, 96, 8, 8)\n",
        "        out = self.layer4(out)  # (B, 192, 4, 4)\n",
        "        \n",
        "        # Global average pooling\n",
        "        out = F.avg_pool2d(out, 4)  # (B, 192, 1, 1)\n",
        "        \n",
        "        # Flatten\n",
        "        out = out.view(out.size(0), -1)  # (B, 192)\n",
        "        \n",
        "        # Classification\n",
        "        out = self.linear(out)  # (B, num_classes)\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18Small(num_classes=10):\n",
        "    \"\"\"\n",
        "    ResNet18Small 모델 생성 함수\n",
        "    \n",
        "    Args:\n",
        "        num_classes: 분류할 클래스 개수\n",
        "        \n",
        "    Returns:\n",
        "        ResNet18Small 모델 (파라미터: ~1.58M)\n",
        "    \"\"\"\n",
        "    return ResNetSmall(BasicBlock, [2, 2, 2, 2], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 ViT-Small 모델"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pair(t):\n",
        "    \"\"\"\n",
        "    단일 값을 튜플로 변환하는 헬퍼 함수\n",
        "    \n",
        "    이미 튜플이면 그대로 반환하고, 아니면 (t, t) 형태로 변환합니다.\n",
        "    주로 image_size, patch_size 등을 처리할 때 사용합니다.\n",
        "    \n",
        "    Args:\n",
        "        t: 정수 또는 튜플\n",
        "        \n",
        "    Returns:\n",
        "        튜플 (t, t) 또는 원본 튜플\n",
        "    \"\"\"\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "\n",
        "\n",
        "class PreNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    LayerNorm을 먼저 적용한 후 함수를 실행하는 래퍼 클래스\n",
        "    \n",
        "    Transformer의 Pre-LayerNorm 구조를 구현합니다.\n",
        "    일반적으로 Attention이나 FeedForward 전에 정규화를 수행합니다.\n",
        "    \n",
        "    Args:\n",
        "        dim: 입력 차원\n",
        "        fn: 적용할 함수 (Attention 또는 FeedForward)\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.fn = fn\n",
        "        \n",
        "    def forward(self, x, **kwargs):\n",
        "        \"\"\"정규화 후 함수 적용\"\"\"\n",
        "        return self.fn(self.norm(x), **kwargs)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer의 Feed-Forward Network (FFN)\n",
        "    \n",
        "    Position-wise fully connected feed-forward network입니다.\n",
        "    두 개의 linear layer와 GELU 활성화 함수로 구성됩니다.\n",
        "    \n",
        "    구조:\n",
        "        Input → Linear(dim → hidden_dim) → GELU → Dropout \n",
        "              → Linear(hidden_dim → dim) → Dropout → Output\n",
        "    \n",
        "    Args:\n",
        "        dim: 입력/출력 차원\n",
        "        hidden_dim: 은닉층 차원 (일반적으로 dim의 4배)\n",
        "        dropout: 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"FFN 적용\"\"\"\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class LSA(nn.Module):\n",
        "    \"\"\"\n",
        "    Locality Self-Attention (LSA)\n",
        "    \n",
        "    일반적인 self-attention과 달리 자기 자신에 대한 attention을 mask하여\n",
        "    지역적 정보(locality)를 강조하는 attention mechanism입니다.\n",
        "    \n",
        "    특징:\n",
        "        - 자기 자신을 보지 못하도록 대각선 mask 적용\n",
        "        - 온도 파라미터(temperature)를 학습 가능하게 설정\n",
        "        - Multi-head attention 구조\n",
        "    \n",
        "    Args:\n",
        "        dim: 입력 차원\n",
        "        heads: attention head 개수\n",
        "        dim_head: 각 head의 차원\n",
        "        dropout: 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        \n",
        "        # 학습 가능한 temperature 파라미터 (스케일링 팩터)\n",
        "        self.temperature = nn.Parameter(torch.log(torch.tensor(dim_head ** -0.5)))\n",
        "        \n",
        "        self.attend = nn.Softmax(dim=-1)\n",
        "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n",
        "        \n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        LSA forward pass\n",
        "        \n",
        "        Args:\n",
        "            x: 입력 텐서 (batch, num_patches, dim)\n",
        "            \n",
        "        Returns:\n",
        "            attention 적용된 출력 (batch, num_patches, dim)\n",
        "        \"\"\"\n",
        "        # Q, K, V 생성\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "        \n",
        "        # Attention score 계산: Q @ K^T * temperature\n",
        "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.temperature.exp()\n",
        "        \n",
        "        # 자기 자신에 대한 attention mask (대각선)\n",
        "        mask = torch.eye(dots.shape[-1], device=dots.device, dtype=torch.bool)\n",
        "        mask_value = -torch.finfo(dots.dtype).max\n",
        "        dots = dots.masked_fill(mask, mask_value)\n",
        "        \n",
        "        # Softmax로 attention weights 계산\n",
        "        attn = self.attend(dots)\n",
        "        \n",
        "        # Attention 적용: weights @ V\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
        "        \n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer Encoder\n",
        "    \n",
        "    여러 개의 Transformer block을 쌓아 구성합니다.\n",
        "    각 block은 LSA + FFN으로 구성되며, residual connection이 적용됩니다.\n",
        "    \n",
        "    구조 (각 block마다):\n",
        "        Input → PreNorm → LSA ─┬→ + → PreNorm → FFN ─┬→ + → Output\n",
        "          └──────────────────┘              └──────────┘\n",
        "          (residual connection)           (residual connection)\n",
        "    \n",
        "    Args:\n",
        "        dim: 입력/출력 차원\n",
        "        depth: Transformer block 개수\n",
        "        heads: attention head 개수\n",
        "        dim_head: 각 head의 차원\n",
        "        mlp_dim: FFN의 은닉층 차원\n",
        "        dropout: 드롭아웃 비율\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        \n",
        "        # depth 만큼 Transformer block 생성\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                PreNorm(dim, LSA(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout=dropout))\n",
        "            ]))\n",
        "            \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Transformer forward pass\n",
        "        \n",
        "        Args:\n",
        "            x: 입력 텐서 (batch, num_patches+1, dim)\n",
        "            \n",
        "        Returns:\n",
        "            변환된 출력 (batch, num_patches+1, dim)\n",
        "        \"\"\"\n",
        "        # 각 Transformer block 통과\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x) + x  # Attention + residual\n",
        "            x = ff(x) + x    # FFN + residual\n",
        "            \n",
        "        return x\n",
        "\n",
        "\n",
        "class SPT(nn.Module):\n",
        "    \"\"\"\n",
        "    Shifted Patch Tokenization (SPT)\n",
        "    \n",
        "    일반적인 patch embedding과 달리, 원본 이미지와 shifted된 이미지들을\n",
        "    함께 사용하여 더 풍부한 patch representation을 생성합니다.\n",
        "    \n",
        "    동작 방식:\n",
        "        1. 원본 이미지 + 상하좌우로 1픽셀 shifted된 4개 이미지 = 총 5개\n",
        "        2. 각각을 patch로 분할 (채널 방향으로 concatenate)\n",
        "        3. Linear projection으로 embedding 차원으로 변환\n",
        "    \n",
        "    Args:\n",
        "        dim: 출력 embedding 차원\n",
        "        patch_size: 패치 크기\n",
        "        channels: 입력 이미지 채널 수 (RGB: 3)\n",
        "    \"\"\"\n",
        "    def __init__(self, *, dim, patch_size, channels=3):\n",
        "        super().__init__()\n",
        "        # 5개 이미지 (원본 + shifted 4개) × channels\n",
        "        patch_dim = patch_size * patch_size * 5 * channels\n",
        "        \n",
        "        self.to_patch_tokens = nn.Sequential(\n",
        "            # 이미지를 패치로 분할\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', \n",
        "                     p1=patch_size, p2=patch_size),\n",
        "            nn.LayerNorm(patch_dim),\n",
        "            nn.Linear(patch_dim, dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        SPT forward pass\n",
        "        \n",
        "        Args:\n",
        "            x: 입력 이미지 (batch, 3, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            패치 임베딩 (batch, num_patches, dim)\n",
        "        \"\"\"\n",
        "        # 상하좌우로 1픽셀 shift (padding으로 구현)\n",
        "        shifts = ((1, -1, 0, 0), (-1, 1, 0, 0), (0, 0, 1, -1), (0, 0, -1, 1))\n",
        "        shifted_x = list(map(lambda shift: F.pad(x, shift), shifts))\n",
        "        \n",
        "        # 원본 + shifted 4개 = 5개를 채널 방향으로 concat\n",
        "        x_with_shifts = torch.cat((x, *shifted_x), dim=1)  # (B, 15, H, W)\n",
        "        \n",
        "        # 패치로 변환 및 embedding\n",
        "        return self.to_patch_tokens(x_with_shifts)\n",
        "\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT) with Shifted Patch Tokenization\n",
        "    \n",
        "    이미지 분류를 위한 Transformer 모델입니다.\n",
        "    SPT를 사용하여 이미지를 패치로 변환하고, Transformer encoder로 처리한 뒤\n",
        "    classification head로 최종 예측을 수행합니다.\n",
        "    \n",
        "    전체 구조:\n",
        "        Input Image (32×32×3)\n",
        "          ↓\n",
        "        SPT (Shifted Patch Tokenization)\n",
        "          ↓\n",
        "        Patch Embeddings (64 patches × dim)\n",
        "          ↓\n",
        "        + CLS Token + Positional Embedding\n",
        "          ↓\n",
        "        Dropout\n",
        "          ↓\n",
        "        Transformer Encoder (depth layers)\n",
        "          ↓\n",
        "        CLS Token 추출 (또는 mean pooling)\n",
        "          ↓\n",
        "        MLP Head (LayerNorm → Linear)\n",
        "          ↓\n",
        "        Classification Output (num_classes)\n",
        "    \n",
        "    Args:\n",
        "        image_size: 입력 이미지 크기 (정사각형)\n",
        "        patch_size: 패치 크기\n",
        "        num_classes: 분류할 클래스 개수\n",
        "        dim: embedding 차원\n",
        "        depth: Transformer block 개수\n",
        "        heads: attention head 개수\n",
        "        mlp_dim: FFN 은닉층 차원\n",
        "        pool: pooling 방식 ('cls' 또는 'mean')\n",
        "        channels: 입력 이미지 채널 수\n",
        "        dim_head: 각 attention head의 차원\n",
        "        dropout: attention/FFN dropout 비율\n",
        "        emb_dropout: embedding dropout 비율\n",
        "    \"\"\"\n",
        "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, \n",
        "                 mlp_dim, pool='cls', channels=3, dim_head=64, dropout=0., emb_dropout=0.):\n",
        "        super().__init__()\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "        \n",
        "        # 이미지가 패치 크기로 나누어떨어지는지 확인\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, \\\n",
        "            'Image dimensions must be divisible by the patch size.'\n",
        "        \n",
        "        # 패치 개수 계산\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        \n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls or mean'\n",
        "        \n",
        "        # TODO 1) patch embedding\n",
        "        self.to_patch_embedding = ____\n",
        "\n",
        "        # TODO 2) cls token, positional embedding\n",
        "        self.pos_embedding = ____\n",
        "        self.cls_token = ____\n",
        "\n",
        "        # TODO 3) dropout, transformer, head\n",
        "        self.dropout = ____\n",
        "        self.transformer = ____\n",
        "        self.pool = pool\n",
        "        self.to_latent = nn.Identity()\n",
        "        self.mlp_head = ____\n",
        "\n",
        "    def forward(self, img):\n",
        "        \"\"\"\n",
        "        ViT forward pass\n",
        "        \n",
        "        Args:\n",
        "            img: 입력 이미지 (batch, 3, H, W)\n",
        "            \n",
        "        Returns:\n",
        "            분류 로짓 (batch, num_classes)\n",
        "        \"\"\"\n",
        "        # TODO 5) patch embedding\n",
        "        x = ____\n",
        "        b, n, _ = x.shape\n",
        "\n",
        "        # TODO 6) cls token concat\n",
        "        cls_tokens = ____\n",
        "        x = ____\n",
        "\n",
        "        # TODO 7) pos embedding + dropout\n",
        "        x = ____\n",
        "        x = ____\n",
        "\n",
        "        # TODO 8) transformer\n",
        "        x = ____\n",
        "\n",
        "        # TODO 9) pooling\n",
        "        x = ____  \n",
        "\n",
        "        # TODO 10) classification head\n",
        "        x = self.to_latent(x)\n",
        "        return ____\n",
        "\n",
        "\n",
        "def create_vit_small():\n",
        "    \"\"\"\n",
        "    ViT-Small 모델 생성 함수\n",
        "    \n",
        "    경량화된 ViT 모델로, 다음과 같은 설정을 사용합니다:\n",
        "        - Embedding dim: 256 (표준 ViT의 768에서 축소)\n",
        "        - Depth: 4 layers (표준 ViT의 12에서 축소)\n",
        "        - Heads: 4 (표준 ViT의 12에서 축소)\n",
        "        - MLP dim: 256 (표준 ViT의 3072에서 축소)\n",
        "    \n",
        "    Returns:\n",
        "        ViT 모델 (파라미터: ~1.66M)\n",
        "    \"\"\"\n",
        "    model = ViT(\n",
        "        image_size=IMAGE_SIZE,      # 32\n",
        "        patch_size=PATCH_SIZE,       # 4\n",
        "        num_classes=NUM_CLASSES,     # 100\n",
        "        dim=256,                      # Embedding 차원\n",
        "        depth=4,                      # Transformer layers\n",
        "        heads=4,                      # Attention heads\n",
        "        mlp_dim=256,                  # FFN hidden dim\n",
        "        dropout=0.1,\n",
        "        emb_dropout=0.1\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_resnet18_small():\n",
        "    \"\"\"\n",
        "    ResNet18Small 모델 생성 함수\n",
        "    \n",
        "    Returns:\n",
        "        ResNet18Small 모델 (파라미터: ~1.58M)\n",
        "    \"\"\"\n",
        "    model = ResNet18Small(num_classes=NUM_CLASSES)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. 학습 및 평가 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, trainloader, criterion, optimizer, device):\n",
        "    \"\"\"한 에포크 학습\"\"\"\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(trainloader, desc='Training')\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({'loss': train_loss/(pbar.n+1), 'acc': 100.*correct/total})\n",
        "    \n",
        "    return train_loss/len(trainloader), 100.*correct/total\n",
        "\n",
        "def evaluate(model, testloader, criterion, device):\n",
        "    \"\"\"모델 평가\"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(testloader, desc='Testing')\n",
        "        for inputs, targets in pbar:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            \n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "            pbar.set_postfix({'loss': test_loss/(pbar.n+1), 'acc': 100.*correct/total})\n",
        "    \n",
        "    return test_loss/len(testloader), 100.*correct/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. 전체 학습 함수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, model_name, num_epochs=NUM_EPOCHS):\n",
        "    \"\"\"모델 학습 전체 프로세스\"\"\"\n",
        "    model = model.to(device)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n",
        "    \n",
        "    # 기록용\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "        'lr': []\n",
        "    }\n",
        "    \n",
        "    best_acc = 0\n",
        "    \n",
        "    print(f'\\n==> Training {model_name}...')\n",
        "    start_time = time.time()\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
        "        \n",
        "        # 학습\n",
        "        train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
        "        \n",
        "        # 평가\n",
        "        test_loss, test_acc = evaluate(model, testloader, criterion, device)\n",
        "        \n",
        "        # 스케줄러 업데이트\n",
        "        scheduler.step()\n",
        "        \n",
        "        # 기록\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(test_acc)\n",
        "        history['lr'].append(optimizer.param_groups[0]['lr'])\n",
        "        \n",
        "        # 결과 출력\n",
        "        print(f'Train Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "        print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc:.2f}%')\n",
        "        print(f'LR: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
        "        \n",
        "        # 베스트 모델 저장\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            print(f'==> Saving best model.. (Acc: {best_acc:.2f}%)')\n",
        "    \n",
        "    total_time = time.time() - start_time\n",
        "    print(f'\\nTraining completed in {total_time/60:.2f} minutes')\n",
        "    print(f'Best Test Accuracy: {best_acc:.2f}%')\n",
        "    \n",
        "    return history, best_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. ViT-Small 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ViT-Small 모델 생성 및 학습\n",
        "vit_model = create_vit_small()\n",
        "print(f'ViT-Small Parameters: {sum(p.numel() for p in vit_model.parameters())/1e6:.2f}M')\n",
        "\n",
        "vit_history, vit_best_acc = train_model(vit_model, 'ViT-Small', num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ResNet18Small 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ResNet18Small 모델 생성 및 학습\n",
        "resnet_model = create_resnet18_small()\n",
        "print(f'ResNet18Small Parameters: {sum(p.numel() for p in resnet_model.parameters())/1e6:.2f}M')\n",
        "\n",
        "resnet_history, resnet_best_acc = train_model(resnet_model, 'ResNet18Small', num_epochs=NUM_EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"\n",
        "    모델의 총 파라미터 수를 계산\n",
        "    \n",
        "    Args:\n",
        "        model: PyTorch 모델\n",
        "        \n",
        "    Returns:\n",
        "        파라미터 수 (정수)\n",
        "    \"\"\"\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def calculate_flops_vit(model, image_size=32, patch_size=4):\n",
        "    \"\"\"\n",
        "    ViT 모델의 FLOPs 계산 (근사값)\n",
        "    \n",
        "    Args:\n",
        "        model: ViT 모델\n",
        "        image_size: 입력 이미지 크기\n",
        "        patch_size: 패치 크기\n",
        "        \n",
        "    Returns:\n",
        "        FLOPs (부동소수점 연산 횟수)\n",
        "    \"\"\"\n",
        "    num_patches = (image_size // patch_size) ** 2\n",
        "    \n",
        "    # Patch embedding: SPT 사용 (5개 이미지 concat)\n",
        "    patch_dim = patch_size * patch_size * 3 * 5\n",
        "    embed_dim = model.to_patch_embedding.to_patch_tokens[-1].out_features\n",
        "    embedding_flops = num_patches * patch_dim * embed_dim\n",
        "    \n",
        "    # Transformer blocks\n",
        "    depth = len(model.transformer.layers)\n",
        "    seq_len = num_patches + 1  # +1 for CLS token\n",
        "    \n",
        "    # Self-attention per block\n",
        "    # Q, K, V projection: 3 * seq_len * embed_dim * embed_dim\n",
        "    # Attention: seq_len * seq_len * embed_dim\n",
        "    # Output projection: seq_len * embed_dim * embed_dim\n",
        "    attn_flops_per_block = (\n",
        "        3 * seq_len * embed_dim * embed_dim +  # QKV projection\n",
        "        seq_len * seq_len * embed_dim +         # Attention computation\n",
        "        seq_len * embed_dim * embed_dim         # Output projection\n",
        "    )\n",
        "    \n",
        "    # FFN per block\n",
        "    # Linear1: seq_len * embed_dim * mlp_dim\n",
        "    # Linear2: seq_len * mlp_dim * embed_dim\n",
        "    mlp_dim = model.transformer.layers[0][1].fn.net[0].out_features\n",
        "    ffn_flops_per_block = (\n",
        "        seq_len * embed_dim * mlp_dim +\n",
        "        seq_len * mlp_dim * embed_dim\n",
        "    )\n",
        "    \n",
        "    transformer_flops = depth * (attn_flops_per_block + ffn_flops_per_block)\n",
        "    \n",
        "    # Classification head\n",
        "    head_flops = embed_dim * model.mlp_head[-1].out_features\n",
        "    \n",
        "    total_flops = embedding_flops + transformer_flops + head_flops\n",
        "    return total_flops\n",
        "\n",
        "\n",
        "def calculate_flops_resnet(model, image_size=32):\n",
        "    \"\"\"\n",
        "    ResNet 모델의 FLOPs 계산 (근사값)\n",
        "    \n",
        "    Args:\n",
        "        model: ResNet 모델\n",
        "        image_size: 입력 이미지 크기\n",
        "        \n",
        "    Returns:\n",
        "        FLOPs (부동소수점 연산 횟수)\n",
        "    \"\"\"\n",
        "    def conv_flops(in_channels, out_channels, kernel_size, h, w, stride=1):\n",
        "        \"\"\"Convolution FLOPs 계산\"\"\"\n",
        "        out_h = h // stride\n",
        "        out_w = w // stride\n",
        "        return kernel_size * kernel_size * in_channels * out_channels * out_h * out_w\n",
        "    \n",
        "    flops = 0\n",
        "    h, w = image_size, image_size\n",
        "    \n",
        "    # Initial conv: 3x3, 3->24\n",
        "    flops += conv_flops(3, 24, 3, h, w)\n",
        "    \n",
        "    # Layer1: 2 blocks, 24->24, stride=1\n",
        "    for _ in range(2):\n",
        "        flops += conv_flops(24, 24, 3, h, w)  # conv1\n",
        "        flops += conv_flops(24, 24, 3, h, w)  # conv2\n",
        "    \n",
        "    # Layer2: 2 blocks, 24->48, stride=2 (first), then 48->48\n",
        "    h, w = h // 2, w // 2\n",
        "    flops += conv_flops(24, 48, 3, h*2, w*2, stride=2)  # first conv1\n",
        "    flops += conv_flops(48, 48, 3, h, w)  # first conv2\n",
        "    flops += conv_flops(24, 48, 1, h*2, w*2, stride=2)  # shortcut\n",
        "    flops += conv_flops(48, 48, 3, h, w)  # second conv1\n",
        "    flops += conv_flops(48, 48, 3, h, w)  # second conv2\n",
        "    \n",
        "    # Layer3: 2 blocks, 48->96, stride=2 (first), then 96->96\n",
        "    h, w = h // 2, w // 2\n",
        "    flops += conv_flops(48, 96, 3, h*2, w*2, stride=2)  # first conv1\n",
        "    flops += conv_flops(96, 96, 3, h, w)  # first conv2\n",
        "    flops += conv_flops(48, 96, 1, h*2, w*2, stride=2)  # shortcut\n",
        "    flops += conv_flops(96, 96, 3, h, w)  # second conv1\n",
        "    flops += conv_flops(96, 96, 3, h, w)  # second conv2\n",
        "    \n",
        "    # Layer4: 2 blocks, 96->192, stride=2 (first), then 192->192\n",
        "    h, w = h // 2, w // 2\n",
        "    flops += conv_flops(96, 192, 3, h*2, w*2, stride=2)  # first conv1\n",
        "    flops += conv_flops(192, 192, 3, h, w)  # first conv2\n",
        "    flops += conv_flops(96, 192, 1, h*2, w*2, stride=2)  # shortcut\n",
        "    flops += conv_flops(192, 192, 3, h, w)  # second conv1\n",
        "    flops += conv_flops(192, 192, 3, h, w)  # second conv2\n",
        "    \n",
        "    # FC layer\n",
        "    flops += 192 * model.linear.out_features\n",
        "    \n",
        "    return flops\n",
        "\n",
        "\n",
        "# 모델 효율성 지표 계산\n",
        "print('==> 모델 효율성 지표 계산 중...\\n')\n",
        "\n",
        "models_info = []\n",
        "\n",
        "# ViT-Small\n",
        "vit_params = count_parameters(vit_model)\n",
        "vit_flops = calculate_flops_vit(vit_model)\n",
        "models_info.append({\n",
        "    'name': 'ViT-Small',\n",
        "    'type': 'ViT',\n",
        "    'params': vit_params,\n",
        "    'flops': vit_flops,\n",
        "    'accuracy': vit_best_acc,\n",
        "    'marker': 'o',\n",
        "    'color': 'blue'\n",
        "})\n",
        "\n",
        "# ResNet18Small\n",
        "resnet_params = count_parameters(resnet_model)\n",
        "resnet_flops = calculate_flops_resnet(resnet_model)\n",
        "models_info.append({\n",
        "    'name': 'ResNet18Small',\n",
        "    'type': 'CNN',\n",
        "    'params': resnet_params,\n",
        "    'flops': resnet_flops,\n",
        "    'accuracy': resnet_best_acc,\n",
        "    'marker': 's',\n",
        "    'color': 'red'\n",
        "})\n",
        "\n",
        "# 결과 출력\n",
        "print('모델 효율성 지표:')\n",
        "print('='*80)\n",
        "for model in models_info:\n",
        "    acc_per_param = model['accuracy'] / model['params']\n",
        "    acc_per_flop = model['accuracy'] / model['flops']\n",
        "    \n",
        "    print(f\"\\n{model['name']} ({model['type']}):\")\n",
        "    print(f\"  - Parameters: {model['params']:,} ({model['params']/1e6:.2f}M)\")\n",
        "    print(f\"  - FLOPs: {model['flops']:,} ({model['flops']/1e6:.2f}M)\")\n",
        "    print(f\"  - Test Accuracy: {model['accuracy']:.2f}%\")\n",
        "    print(f\"  - Accuracy per Param: {acc_per_param:.6e}\")\n",
        "    print(f\"  - Accuracy per FLOP:  {acc_per_flop:.6e}\")\n",
        "print('='*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CIFAR-10 학습 결과 그래프\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "epochs = range(1, NUM_EPOCHS + 1)\n",
        "\n",
        "# Test Accuracy\n",
        "axes[0].plot(epochs, vit_history['test_acc'], 'b-', label='ViT-Small', linewidth=2, marker='o', markersize=4)\n",
        "axes[0].plot(epochs, resnet_history['test_acc'], 'r-', label='ResNet18Small', linewidth=2, marker='s', markersize=4)\n",
        "axes[0].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
        "axes[0].set_ylabel('Test Accuracy (%)', fontsize=13, fontweight='bold')\n",
        "axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(fontsize=12)\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Accuracy per FLOPs (효율성)\n",
        "vit_flops_per_epoch = vit_flops  # FLOPs per forward pass\n",
        "resnet_flops_per_epoch = resnet_flops\n",
        "\n",
        "axes[1].plot(epochs, [acc / (vit_flops_per_epoch / 1e9) for acc in vit_history['test_acc']], \n",
        "             'b-', label='ViT-Small', linewidth=2, marker='o', markersize=4)\n",
        "axes[1].plot(epochs, [acc / (resnet_flops_per_epoch / 1e9) for acc in resnet_history['test_acc']], \n",
        "             'r-', label='ResNet18Small', linewidth=2, marker='s', markersize=4)\n",
        "axes[1].set_xlabel('Epoch', fontsize=13, fontweight='bold')\n",
        "axes[1].set_ylabel('Accuracy / GFLOPs', fontsize=13, fontweight='bold')\n",
        "axes[1].set_title('Computational Efficiency (Higher is Better)', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(fontsize=12)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print('\\n==> Training comparison graph saved as training_comparison.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. 최종 결과 요약"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('\\n' + '='*60)\n",
        "print('CIFAR-10 Training Results'.center(60))\n",
        "print('='*60)\n",
        "print(f'\\nViT-Small:')\n",
        "print(f'  - Parameters: {sum(p.numel() for p in vit_model.parameters())/1e6:.2f}M')\n",
        "print(f'  - Best Test Accuracy: {vit_best_acc:.2f}%')\n",
        "print(f'  - Final Train Accuracy: {vit_history[\"train_acc\"][-1]:.2f}%')\n",
        "print(f'  - Final Test Accuracy: {vit_history[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print(f'\\nResNet18Small:')\n",
        "print(f'  - Parameters: {sum(p.numel() for p in resnet_model.parameters())/1e6:.2f}M')\n",
        "print(f'  - Best Test Accuracy: {resnet_best_acc:.2f}%')\n",
        "print(f'  - Final Train Accuracy: {resnet_history[\"train_acc\"][-1]:.2f}%')\n",
        "print(f'  - Final Test Accuracy: {resnet_history[\"test_acc\"][-1]:.2f}%')\n",
        "\n",
        "print('\\n' + '='*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. 모델 추론 결과 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_predictions(vit_model, resnet_model, testset, classes, mean, std, num_samples=10):\n",
        "    \"\"\"Model prediction visualization (2x5 grid)\"\"\"\n",
        "    vit_model.eval()\n",
        "    resnet_model.eval()\n",
        "    \n",
        "    indices = np.random.choice(len(testset), num_samples, replace=False)\n",
        "    \n",
        "    # 2x5 grid\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axes = axes.ravel()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, sample_idx in enumerate(indices):\n",
        "            img, true_label = testset[sample_idx]\n",
        "            img_batch = img.unsqueeze(0).to(device)\n",
        "            \n",
        "            vit_output = vit_model(img_batch)\n",
        "            resnet_output = resnet_model(img_batch)\n",
        "            \n",
        "            vit_probs = F.softmax(vit_output, dim=1)\n",
        "            resnet_probs = F.softmax(resnet_output, dim=1)\n",
        "            \n",
        "            vit_pred = vit_output.argmax(dim=1).item()\n",
        "            resnet_pred = resnet_output.argmax(dim=1).item()\n",
        "            \n",
        "            vit_conf = vit_probs[0, vit_pred].item() * 100\n",
        "            resnet_conf = resnet_probs[0, resnet_pred].item() * 100\n",
        "            \n",
        "            img_show = denormalize(img, mean, std)\n",
        "            img_show = img_show.permute(1, 2, 0).numpy()\n",
        "            img_show = np.clip(img_show, 0, 1)\n",
        "            \n",
        "            axes[idx].imshow(img_show)\n",
        "            axes[idx].axis('off')\n",
        "            \n",
        "            # Short class names for CIFAR-100\n",
        "            if len(classes) > 10:\n",
        "                true_class = f'C{true_label}'\n",
        "                vit_class = f'C{vit_pred}'\n",
        "                resnet_class = f'C{resnet_pred}'\n",
        "            else:\n",
        "                true_class = classes[true_label]\n",
        "                vit_class = classes[vit_pred]\n",
        "                resnet_class = classes[resnet_pred]\n",
        "            \n",
        "            vit_color = 'green' if vit_pred == true_label else 'red'\n",
        "            resnet_color = 'green' if resnet_pred == true_label else 'red'\n",
        "            \n",
        "            title = f'GT: {true_class}\\nViT: {vit_class}\\nRN: {resnet_class}'\n",
        "            axes[idx].set_title(title, fontsize=9, pad=5)\n",
        "            \n",
        "            # Confidence on image\n",
        "            bbox_vit = dict(boxstyle='round,pad=0.2', facecolor=vit_color, alpha=0.6)\n",
        "            axes[idx].text(0.05, 0.95, f'{vit_conf:.0f}%', transform=axes[idx].transAxes,\n",
        "                          fontsize=8, verticalalignment='top', bbox=bbox_vit, color='white', fontweight='bold')\n",
        "            \n",
        "            bbox_resnet = dict(boxstyle='round,pad=0.2', facecolor=resnet_color, alpha=0.6)\n",
        "            axes[idx].text(0.95, 0.95, f'{resnet_conf:.0f}%', transform=axes[idx].transAxes,\n",
        "                          fontsize=8, verticalalignment='top', horizontalalignment='right', \n",
        "                          bbox=bbox_resnet, color='white', fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_predictions.png', dpi=200, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    print('\\n==> Model predictions saved as model_predictions.png')\n",
        "\n",
        "\n",
        "# Visualization\n",
        "print('\\n==> Visualizing model predictions...')\n",
        "visualize_predictions(vit_model, resnet_model, testset, testset.classes, MEAN, STD, num_samples=10)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "3.10.13",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
